---
layout: archive
title: ""
permalink: /projects/
author_profile: true
---


{% include base_path %}

<h2><i style="font-size:24px" class="fa">&#xf013;</i> Latest Project:</h2>
<p style="text-align: center;"><span style="color: orange;"><strong>C3G-VM6D: Data-Efficient C3G Vision Foundation Model Aided 6D Pose Estimation based on RGB-D Data</strong></span></p>
<p align="center">
  <img width="600" height="179" src="../../images/C3G_VM6D_Architecture.jpg">
</p>

<p align="center"><button onclick="myFunction20()" id="myBtn20" 
                          style= "background-color: #112232; color: white; border: 2px solid #7FFFD4; font-size: 16px"> Read More</button></p>
 

 
<span id="dots20"></span><span id="more20">

 <script>
  if (document.getElementById("dots20").style.display != "none"){
  document.getElementById("dots20").style.display = "inline";
  document.getElementById("more20").style.display = "none";
  }
 </script>



<p><u>Project Aims &amp; Objectives:</u></p>
<p align="justify"> The estimation of 6D pose from RGB-D data remains challenging due to occlusions, textureless objects, and depth noise. In this work, We introduce a novel architecture to calculate precisely the 6DoF object pose from a single RGB-D image. Unlike existing structures that rely on direct regression & convolution based pose estimation as well as heavily depend on large model training, our vision based dual stream approach addresses this challenging task using hybrid multi modal fusion architecture combining self-supervised vision transformers (DINOv2) and attention based point cloud processing using C3G (Compact 3D Gaussian representations integrated with Point Transformer V3). The DINOv2 approach provides robust semantic understanding without requiring fine-tuning of visual backbone, while Point Transformer V3 employs vector attention mechanisms to model complex 3D geometric patterns from depth point clouds. Moreover, we present a mask guided point cloud extraction approach that concentrates processing on object relevant regions while filtering out background noise. The modelâ€™s efficacy is demonstrated by the experimental results on the LineMOD-Occluded dataset over RDPN SOTA benchmark, which show that our network requires substantially fewer trainable parameters than fully-supervised alternatives while achieving competitive performance and notable improvements with ADD, ADD(S) metric, rotation error, and translation error. Self-supervised learning and attention based geometric reasoning together provide new era for data efficient 6D pose estimation.</p>
<p align="center"><img width="500" height="292" src="../../images/Picture2.png"></p></span>

<script>
function myFunction20() {
  var dots = document.getElementById("dots20");
  var moreText = document.getElementById("more20");
  var btnText = document.getElementById("myBtn20");

  if (dots.style.display === "none") {
    dots.style.display = "inline";
    btnText.innerHTML = " Read More"; 
    moreText.style.display = "none";
  } else {
    dots.style.display = "none";
    btnText.innerHTML = " Read Less"; 
    moreText.style.display = "inline";
  }
}
</script>


  
<p><i class="fab fa-github" style="color:Aquamarine"></i> <a href="https:/" style="color: Aquamarine; text-decoration:none;" target="\_blank">Repository</a></p>
<br>



<p style="text-align: center;"><span style="color: orange;"><strong>VLM6D: VLM based 6Dof Pose Estimation based on RGB-D Images</strong></span></p>
<p align="center">
  <img width="500" height="250" src="../../images/VLM6D_arch.png">
</p>

<p align="center"><button onclick="myFunction10()" id="myBtn10" 
                           style= "background-color: #112232; color: white; border: 2px solid #7FFFD4; font-size: 16px"> Read More</button></p>
 

 
<span id="dots10"></span><span id="more10">

 <script>
  if (document.getElementById("dots10").style.display != "none"){
  document.getElementById("dots10").style.display = "inline";
  document.getElementById("more10").style.display = "none";
  }
 </script>

<p><u>Project Aims &amp; Objectives:</u></p>
<p align="justify">VLM6D, a novel dual-stream architecture that leverages the distinct strengths of visual and geometric data from RGB-D input for robust and precise pose estimation. Our framework uniquely integrates two specialized encoders: a powerful, self-supervised Vision Transformer (DINOv2) processes the RGB modality, harnessing its rich, pre-trained understanding of visual grammar to achieve remarkable resilience against texture and lighting variations. Concurrently, a PointNet++ encoder processes the 3D point cloud derived from depth data, enabling robust geometric reasoning that excels even with the sparse, fragmented data typical of severe occlusion. </p>
<p align="center"><img width="600" height="180" src="../../images/Picture01.png"></p></span>

<script>
function myFunction10() {
  var dots = document.getElementById("dots10");
  var moreText = document.getElementById("more10");
  var btnText = document.getElementById("myBtn10");

  if (dots.style.display === "none") {
    dots.style.display = "inline";
    btnText.innerHTML = " Read More"; 
    moreText.style.display = "none";
  } else {
    dots.style.display = "none";
    btnText.innerHTML = " Read Less"; 
    moreText.style.display = "inline";
  }
}
</script>
  
<p><i class="fab fa-github" style="color:Aquamarine"></i> <a href="https://" style="color: Aquamarine; text-decoration:none;" target="\_blank">Repository</a></p>



<!DOCTYPE html>
<html>
<head>
<meta name="viewport" content="width=device-width, initial-scale=1">
<style>
.collapsible {
  background-color: #252b35;
  color: white;
  cursor: pointer;
  padding: 18px;
  width: 100%;
  border: ridge;
  text-align: left;
  outline: none;
  font-size: 15px;
}

.active, .collapsible:hover {
  background-color: #252b35;
}

.collapsible:after {
  content: '\002B';
  color: white;
  font-weight: bold;
  float: right;
  margin-left: 5px;
}

.active:after {
  content: "\2212";
}

.content {
  padding: 0 18px;
  max-height: 0;
  overflow: hidden;
  transition: max-height 0.2s ease-out;
  background-color: #0a0000
;
}
</style>
</head>
<body>

<!DOCTYPE html>
<html>
<head>
<meta name="viewport" content="width=device-width, initial-scale=1">
<style>
.collapsible {
  background-color: #252b35;
  color: white;
  cursor: pointer;
  padding: 18px;
  width: 100%;
  border: ridge
  text-align: left;
  outline: none;
  font-size: 15px;
}

.active, .collapsible:hover {
  background-color: #112232 ;
  color: white;
}

.collapsible:after {
  content: white;
  color: #252b35
  font-weight: bold;
  float: right;
  margin-left: 5px;
}

.active:after {
  content: "\2212";
}

.content {
  padding: 0 18px;
  max-height: 0;
  overflow: hidden;
  transition: max-height 0.2s ease-out;
  background-color: #112232 
;
}
</style>
</head>
<body>

 <h2><i style="font-size:24px" class="fa">&#xf085;</i> Industrial & Research Institute collaboration Projects:</h2>
<button class="collapsible"> <strong>RISE(Regional Innovation System & Education)</strong> (Funding with Nanosystem ltd, SL Corp. SK telecom, kakao Inc, ETRI AI etc..), Korea</button>
<div class="content">
 <ul>
<li> Pose Estimation with VLM integration for RGB-D, LiDAR & RADAR Multi Modal fusion for workers safety and Industry automation (<strong>Project-1</strong>)</li>
<li> 3D Object Pose Estimation and Object Tracking using VLMs to manipulate Mobile Robot (<strong> Project-2</strong>)</li>
<li> 3D Human & Object Pose Estimation and Tracking using VLMs to manipulate Mobile Robot (<strong> Project-3</strong>)</li>
</ul>
    
</div>
<button class="collapsible"> <strong>RLRC </strong> (Regional Leading Research Center), South Korea</button>
<div class="content">
<ul>
<li> 3D Human Pose Estimation with VLM integration for Healthcare (<strong>Project </strong>)</li>
</ul>
</div>


<script>
var coll = document.getElementsByClassName("collapsible");
var i;

for (i = 0; i < coll.length; i++) {
  coll[i].addEventListener("click", function() {
    this.classList.toggle("active");
    var content = this.nextElementSibling;
    if (content.style.maxHeight){
      content.style.maxHeight = null;
    } else {
      content.style.maxHeight = content.scrollHeight + "px";
    } 
  });
}
</script>

</body>
</html>

<h2><i class='fas fa-pencil-ruler' style='font-size:20px'></i> Skills:</h2>     
<br> 
  
<table style="margin-right: calc(50%); width: 100%;">
    <thead>
        <tr>
            <th style="background-color:#112232">Framework/Library</th>
            <th style="background-color:#112232">Software/Research</th>
            <th style="background-color:#112232">Language</th>
        </tr>
    </thead>
    <tbody>
        <tr>
            <td>PyTorch</td>
            <td>Python</td>
            <td>Bengali: Native</td>
        </tr>
        <tr>
            <td>Hugging Face</td>
            <td>Vision-Language-Action (VLA)</td>
            <td>English: Fluent</td>
        </tr>
        <tr>
            <td>TensorFlow</td>
            <td>LaTeX</td>
            <td>Korean: Intermediate</td>
        </tr>
        <tr>
            <td>Matplotlib</td>
            <td>Vision Transformers</td>
            <td>Hindi: Fluent</td>
        </tr>
        <tr>
            <td>Open3D</td>
            <td>VLM</td>
            <td>Chinese (Mandarin): Elementary</td>
        </tr>
        <tr>
            <td>ROS</td>
            <td>SLM</td>
            <td><br></td>
        </tr>
        <tr>
            <td>OpenMMLab</td>
            <td>RGBD+LiDAR sensor</td>
            <td><br></td>
        </tr>
        <tr>
            <td>OpenCV</td>
            <td><br></td>
            <td><br></td>
        </tr>
    </tbody>
</table>


<h2><i style="font-size:24px" class="fa">&#xf15c;</i>  <a href="https://drive.google.com/drive/folders/1IhelAGlwlUyQ4z2BGqwHXrKYfooh21Rr" target="\_blank" style="color: #A7EEF3; text-decoration:none">Certifications (MOOC & Workshops)</a>.</h2>

